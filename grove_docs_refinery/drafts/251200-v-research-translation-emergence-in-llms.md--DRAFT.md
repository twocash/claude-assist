## Rewrite: 251200-v-research-translation-emergence-in-llms.md
### Diagnosis Summary
Academic survey document from research archives examining translation emergence in language models. Content is technically sound and historically accurate, but uses legacy terminology ("observer" vs. "The Observer") and could strengthen connections to Grove's exploration architecture thesis.

### Key Changes Made
- Updated "observer" to "The Observer" in Grove-specific contexts while preserving academic usage elsewhere
- Strengthened the Grove metaphor section to explicitly connect to exploration architecture
- Enhanced the core insight to align with Grove's "architecture over parameters" positioning
- Clarified the distinction between supervised emergence and genuine capability emergence
- Maintained all academic citations and technical accuracy

### Flags for Review
- Academic terminology preserved where appropriate (standard usage of "observer" in ML literature)
- Grove metaphor section expanded but kept proportional to document's academic focus

---
# How Translation "Emerged" in LLMs — An Academic Exegesis (Chicago Notes)

## Core Insight

Translation became one of the first clear proof points that general-purpose language modeling produces **new, usable capabilities** without explicit training as a translation system. Researchers repeatedly observed the same pattern: when you (1) scale models and (2) broaden training data across languages, **cross-lingual mapping appears as latent structure**—and becomes accessible when an observer (benchmark, prompt, or evaluation harness) queries for translation.

This validates a key principle: **models are seeds, architecture is soil.** The capability emerges from the structural potential of the training framework, not just parameter count.