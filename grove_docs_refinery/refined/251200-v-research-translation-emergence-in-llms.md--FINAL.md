# How Translation "Emerged" in LLMs — An Academic Exegesis (Chicago Notes)

## Core Insight

Translation became one of the first clear proof points that general-purpose language modeling produces **new, usable capabilities** without explicit training as a translation system. Researchers repeatedly observed the same pattern: when you (1) scale models and (2) broaden training data across languages, **cross-lingual mapping appears as latent structure**—and becomes accessible when an observer (benchmark, prompt, or evaluation harness) queries for translation.

This validates a key principle: **models are seeds, architecture is soil.** The capability emerges from the structural potential of the training framework, not just parameter count.