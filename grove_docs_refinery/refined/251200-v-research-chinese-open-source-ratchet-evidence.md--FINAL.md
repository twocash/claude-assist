# Chinese Open-Source AI: Ratchet Acceleration Evidence

*Research Brief | December 2025*

**Abstract**

Chinese AI laboratories have emerged as an unexpected acceleration mechanism for local AI capability diffusion. Between December 2024 and July 2025, DeepSeek-V3 and Moonshot's Kimi K2 achieved GPT-4-class performance at 5% of historical training costs ($5-6M vs. $100M+) while releasing full model weights under permissive open-source licenses. This pattern introduces a parallel propagation channel that compresses the Ratchet thesis's projected 21-month frontier-to-local lag, making frontier-adjacent capabilities immediately available for consumer deployment. The trend is driven by competitive dynamics in China's AI ecosystem, training efficiency breakthroughs (FP8 mixed precision, mixture-of-experts architectures), and hardware constraints that paradoxically incentivize optimization. For Grove's hybrid architecture, this acceleration improves the technical risk profile by raising the local capability floor faster than baseline projections, potentially enabling more cognitive operations to remain local at launch while reducing cloud subsidy requirements during the efficiency-enlightenment loop period.

**Classification:** Strategic Intelligence for Grove Architecture Planning

Jim Calhoun 
The-Grove.ai
Copyright 2025